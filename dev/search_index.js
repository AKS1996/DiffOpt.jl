var documenterSearchIndex = {"docs":
[{"location":"solve-QP/#Solving-QP-primal","page":"Solving a QP","title":"Solving QP primal","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"using Random\nusing MathOptInterface\nusing Dualization\nusing OSQP\n\nconst MOI = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"n = 20 # variable dimension\nm = 15 # no of inequality constraints\np = 15; # no of equality constraints","category":"page"},{"location":"solve-QP/#create-a-non-trivial-QP-problem","page":"Solving a QP","title":"create a non-trivial QP problem","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"textmin  frac12x^TQx + q^Tx","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"textst  Gx = h","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"Ax = b ","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"x̂ = rand(n)\nQ = rand(n, n)\nQ = Q'*Q # ensure PSD\nq = rand(n)\nG = rand(m, n)\nh = G*x̂ + rand(m)\nA = rand(p, n)\nb = A*x̂;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nx = MOI.add_variables(model, n);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# define objective\n\nquad_terms = MOI.ScalarQuadraticTerm{Float64}[]\nfor i in 1:n\n    for j in i:n # indexes (i,j), (j,i) will be mirrored. specify only one kind\n        push!(quad_terms, MOI.ScalarQuadraticTerm(Q[i,j],x[i],x[j]))\n    end\nend\n\nobjective_function = MOI.ScalarQuadraticFunction(MOI.ScalarAffineTerm.(q, x),quad_terms,0.0)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarQuadraticFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# maintain constrain to index map - will be useful later\nconstraint_map = Dict()\n\n# add constraints\nfor i in 1:m\n    ci = MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(G[i,:], x), 0.),MOI.LessThan(h[i]))\n    constraint_map[ci] = i\nend\n\nfor i in 1:p\n    ci = MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(A[i,:], x), 0.),MOI.EqualTo(b[i]))\n    constraint_map[ci] = i\nend","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"MOI.optimize!(model)","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"x̄ = MOI.get(model, MOI.VariablePrimal(), x);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# objective value (predicted vs actual) sanity check\n@assert 0.5*x̄'*Q*x̄ + q'*x̄  <= 0.5*x̂'*Q*x̂ + q'*x̂   ","category":"page"},{"location":"solve-QP/#find-and-solve-dual-problem","page":"Solving a QP","title":"find and solve dual problem","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"primal dual\ntextmin  frac12x^TQx + q^Tx textmax  -frac12y^TQ^-1y - u^Th - v^Tb\ntextst  Gx = h textst   u geq 0 u in R^m v in R^n\nAx = b y = q + G^Tu + A^Tv","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"Each primal variable becomes a dual constraint\nEach primal constraint becomes a dual variable","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# NOTE: can't use Ipopt\n# Ipopt.Optimizer doesn't supports accessing MOI.ObjectiveFunctionType\n\njoint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# check if strong duality holds\n@assert abs(MOI.get(model, MOI.ObjectiveValue()) - MOI.get(dual_model, MOI.ObjectiveValue())) <= 1e-1","category":"page"},{"location":"solve-QP/#derive-and-verify-KKT-conditions","page":"Solving a QP","title":"derive and verify KKT conditions","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"is_equality(set::S) where {S<:MOI.AbstractSet} = false\nis_equality(set::MOI.EqualTo{T}) where T = true\n\nmap = primal_dual_map.primal_con_dual_var;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"complimentary slackness: mu_i(Gbar x -h)_i=0 qquad text where  i=1m","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"for con_index in keys(map)\n    # NOTE: OSQP.Optimizer doesn't allows access to MOI.ConstraintPrimal\n    #       That's why I defined a custom map \n    \n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ   = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    \n    if !is_equality(set)\n        # μ[i]*(Gx - h)[i] = 0\n        i = constraint_map[con_index]\n        \n        # println(μ,\" - \",G[i,:]'*x̄, \" - \",h[i])\n        # TODO: assertion fails \n        @assert μ*(G[i,:]'*x̄ - h[i]) < 1e-1  \n    end\nend","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"primal feasibility:  (Gbar x -h)_i=0 qquad text where  i=1m (Abar x -b)_j=0 qquad text where  j=1p","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"dual feasibility:  mu_i geq 0 qquad text where  i=1m","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"for con_index in keys(map)\n    # NOTE: OSQP.Optimizer doesn't allows access to MOI.ConstraintPrimal\n    #       That's why I defined a custom map \n    \n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ   = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    i = constraint_map[con_index]\n    \n    if is_equality(set)\n        # (Ax - h)[i] = 0\n        @assert abs(A[i,:]'*x̄ - b[i]) < 1e-2\n    else\n        # (Gx - h)[i] = 0\n        @assert G[i,:]'*x̄ - h[i] < 1e-2\n        \n        # μ[i] >= 0\n        # TODO: assertion fails \n        @assert μ > -1e-2\n    end\nend","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"","category":"page"},{"location":"solve-LP/#Solving-LP-primal","page":"Solving an LP","title":"Solving LP primal","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"using Random\nusing GLPK\nusing MathOptInterface\nusing Dualization\n\nconst MOI  = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"D = 10  # variable dimension\nN = 20; # no of inequality constraints","category":"page"},{"location":"solve-LP/#create-a-non-trivial-LP-problem","page":"Solving an LP","title":"create a non-trivial LP problem","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"textmin  c^Tx","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"textst  Ax leq b","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"$$x \\geq 0, x \\in R^D$$","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"s = rand(N)\ns = 2*s.-1\nλ = max.(-s, 0)\ns = max.(s, 0)\nx̂ = rand(D)\nA = rand(N, D)\nb = A*x̂ .+ s\nc = -A'*λ;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# can feed dual problem to optimizer like this:\n# model = MOI.instantiate(dual_optimizer(GLPK.Optimizer), with_bridge_type=Float64)\n\nmodel = GLPK.Optimizer()\nx = MOI.add_variables(model, D)\n\n# define objective\nobjective_function = MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(c, x), 0.0)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarAffineFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# will be useful later\nconstraint_indices = []\n\n# set constraints\nfor i in 1:N\n    push!(constraint_indices, MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(A[i,:], x), 0.),MOI.LessThan(b[i])))\nend\n\nfor i in 1:D\n    push!(constraint_indices, MOI.add_constraint(model,MOI.SingleVariable(x[i]),MOI.GreaterThan(0.)))\nend","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"MOI.optimize!(model)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"x̄ = MOI.get(model, MOI.VariablePrimal(), x);  # solution","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"@assert abs(c'x̄ - c'x̂) <= 1e-8   # sanity check","category":"page"},{"location":"solve-LP/#find-and-solve-dual-problem","page":"Solving an LP","title":"find and solve dual problem","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"primal dual\ntextmin  c^Tx textmax  b^Ty\ntextst  Ax leq b textst  A^Ty geq c\nx geq 0 y leq 0","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"Each primal variable becomes a dual constraint\nEach primal constraint becomes a dual variable","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"joint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = GLPK.Optimizer()\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# NOTE: You can obtain components of the dual model individually by -\n# dual_objective = dual_model_like.objective  # b'y\n# dual_variable_indices = [primal_dual_map.primal_con_dual_var[x][1] for x in constraint_indices]\n# dual_constraint_indices = [primal_dual_map.primal_var_dual_con[i] for i in x];\n\n# ŷ = MOI.get(dm, MOI.VariablePrimal(), dual_variable_indices)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# check if strong duality holds\n@assert abs(MOI.get(model, MOI.ObjectiveValue()) - MOI.get(dual_model, MOI.ObjectiveValue())) <= 1e-8","category":"page"},{"location":"solve-LP/#derive-and-verify-KKT-conditions","page":"Solving an LP","title":"derive and verify KKT conditions","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"complimentary slackness: mu_i(Abar x -b)_i=0quad mu_j+N bar x_j =0 qquad text where  i=1N j = 1D","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"is_less_than(set::S) where {S<:MOI.AbstractSet} = false\nis_less_than(set::MOI.LessThan{T}) where T = true\n\nmap = primal_dual_map.primal_con_dual_var\nfor con_index in keys(map)\n    con_value = MOI.get(model, MOI.ConstraintPrimal(), con_index)\n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ         = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    \n    if is_less_than(set)\n        # μ[i]*(Ax - b)[i] = 0\n        @assert μ*(con_value - set.upper) < 1e-10\n    else\n        # μ[j]*x[j] = 0\n        @assert μ*(con_value - set.lower) < 1e-10\n    end\nend","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"","category":"page"},{"location":"intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"An optimization problem is the problem of finding the best solution from all feasible solutions. The standard form of an optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x)operatorname subjectto g_i(x)leq 0quad i=1dots mh_j(x)=0quad j=1dots p\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Note that finding solution to most of the optimization problems is computationally intractable. Here we consider a subset of those problems called convex optimization problems, which admit polynomial time solutions. The standard form of a convex optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x)operatorname subjectto g_i(x)leq 0quad i=1dots mA x = b\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"where f and g_i are convex functions.","category":"page"},{"location":"intro/#Parameterized-problems","page":"Introduction","title":"Parameterized  problems","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"In practice, convex optimization problems include parameters, apart from the decision variables, which determines the structure of the problem itself i.e. the objective function and constraints. Hence they affect the solution too. A general form of a parameterized convex optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x theta)operatorname subjectto g_i(x theta)leq 0quad i=1dots mA(theta) x = b(theta)\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"where theta is the parameter. In different fields, these parameters go by different names:","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Hyperparameters in machine learning\nRisk aversion or other backtesing parameters in financial modelling\nParameterized systems in control theory","category":"page"},{"location":"intro/#What-do-we-mean-by-differentiating-a-parameterized-optimization-program?-Why-do-we-need-it?","page":"Introduction","title":"What do we mean by differentiating a parameterized optimization program? Why do we need it?","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Often, parameters are chosen and tuned by hand - an iterative process - and the structure of the problem is crafted manually. But it is possible to do an automatic gradient based tuning of parameters.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Consider solution of the parametrized optimization problem, x(theta),","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginsplit\nbeginarray lll\nx^*(theta)= underset xoperatorname argmin  f(x theta)\n              operatorname subjectto  g_i(x theta)leq 0quad i=1dots m\n                                           A(theta) x = b(theta)\nendarray\nendsplit","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"which is the input of l(x^*(theta)), a loss function. Our goal is to choose the best parameter theta so that l is optimized. Here, l(x^*(theta)) is the objective function and theta is the decision variable. In order to apply a gradient-based strategy to this problem, we need to differentiate l with respect to theta.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fracpartial l(x^*(theta))partial theta = fracpartial l(x^*(theta))partial x^*(theta)  fracpartial x^*(theta)partial theta","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"By implicit function theorem, this translates to differentiating the program data, i.e. functions f, g_i(x) and matrices A, b, with respect to theta.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"This is can be achieved in two steps or passes:","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Forward pass - Given an initial value of theta, solves the optimization problem to find x^*(theta)\nBackward pass - Given x^*, differentiate and find fracpartial x^*(theta)partial theta","category":"page"},{"location":"solve-conic-1/#Solving-conic-with-PSD-and-SOC-constraints","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"Consider an example program ","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"beginsplit\nbeginarray llcc\nmboxminimize  \nleftlangle\nleft\nbeginarray ccc\n        2  1  0  \n        1  2  1  \n        0  1  2\n   endarray\n   right\n        X rightrangle\n        + x_0        \n          mboxsubject to  \n          leftlangle\n          left\n          beginarray ccc\n          1  0  0  \n          0  1  0  \n          0  0  1\n          endarray\n          right\n          X rightrangle\n          + x_0  =   1   \n            \n            leftlangle\n            left\n            beginarrayccc\n            1  1  1  \n            1  1  1  \n            1  1  1\n            endarray\n            right\n            X rightrangle + x_1 + x_2\n             =  12  \n             (x_0 x_1 x_2) in mathbbQ^3 text or  x_0 geq sqrtx_1^2 + x_2^2 \n             X succeq 0 X in mathbbS^3_+\nendarray\nendsplit","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"where","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"mathbbS^n_+ =\nleftlbrace\nX in mathbbS^n z^T X z geq 0 quad forall z in mathbbR^n\nrightrbrace","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"Refered from Mosek examples: https://docs.mosek.com/9.2/toolbox/tutorial-sdo-shared.html#example-sdo1","category":"page"},{"location":"solve-conic-1/#Equivalent-DiffCP-program-to-differentiate","page":"Solving conic with PSD and SOC constraints","title":"Equivalent DiffCP program to differentiate","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"import numpy as np\nimport cvxpy as cp\nfrom scipy import sparse\nimport diffcp\n\nA = sparse.csc_matrix((11+1,7+1), dtype=np.float64)\nA[2 , 1]  =  1.0\nA[3 , 1]  =  -1.0\nA[9 , 1]  =  -0.45\nA[10, 1]  =  0.45\nA[11, 1]  =  -0.45\nA[2 , 2]  =  1.0\nA[4 , 2]  =  -1.0\nA[9 , 2]  =  -0.8\nA[10, 2]  =  0.318198\nA[11, 2]  =  -0.1\nA[2 , 3]  =  1.0\nA[5 , 3]  =  -1.0\nA[9 , 3]  =  -0.9\nA[2 , 4]  =  1.0\nA[6 , 4]  =  -1.0\nA[9 , 4]  =  -0.225\nA[2 , 5]  =  1.0\nA[7 , 5]  =  -1.0\nA[9 , 5]  =  -0.1125\nA[10, 5]  =  0.1125\nA[11, 5]  =  -0.1125\nA[2 , 6]  =  1.0\nA[8 , 6]  =  -1.0\nA[11, 6]  =  -0.225\nA[9 , 7]  =  1.0\nA[11, 7]  =  1.0\n\nA = A[1:, 1:]\n\n# equivalent to: https://github.com/jump-dev/MathOptInterface.jl/blob/master/src/Test/contconic.jl#L2575\n\ncone_dict = {\n    diffcp.POS: 7,\n    diffcp.PSD: [2],\n    diffcp.ZERO: 1\n}\n\nb = np.array([0.0, 10.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0])\nc = np.array([-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0])\n\nx, y, s, D, DT = diffcp.solve_and_derivative(A, b, c, cone_dict)\nprint(x) # MOI.VariablePrimal\nprint(s) # MOI.ConstraintPrimal\nprint(y) # MOI.ConstraintDual\n\n\ndx, dy, ds = D(sparse.csc_matrix(np.ones((11,7))), np.ones(11), np.ones(7))\nprint(dx)\nprint(ds)\nprint(dy)","category":"page"},{"location":"solve-conic-1/#Equivalent-DiffOpt-program","page":"Solving conic with PSD and SOC constraints","title":"Equivalent DiffOpt program","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"using SCS\nusing DiffOpt\nusing MathOptInterface\n\nconst MOI = MathOptInterface;\n\n\nmodel = diff_optimizer(SCS.Optimizer)\nMOI.set(model, MathOptInterface.Silent(), true)\n\nδ = √(1 + (3*√2+2)*√(-116*√2+166) / 14) / 2\nε = √((1 - 2*(√2-1)*δ^2) / (2-√2))\ny2 = 1 - ε*δ\ny1 = 1 - √2*y2\nobj = y1 + y2/2\nk = -2*δ/ε\nx2 = ((3-2obj)*(2+k^2)-4) / (4*(2+k^2)-4*√2)\nα = √(3-2obj-4x2)/2\nβ = k*α\n\nX = MOI.add_variables(model, 6)\nx = MOI.add_variables(model, 3)\n\nvov = MOI.VectorOfVariables(X)\n\ncX = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction{Float64}(vov), MOI.PositiveSemidefiniteConeTriangle(3)\n)\n\ncx = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction{Float64}(MOI.VectorOfVariables(x)), MOI.SecondOrderCone(3)\n)\n\nc1 = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction(\n        MOI.VectorAffineTerm.(1:1, MOI.ScalarAffineTerm.([1., 1., 1., 1.], [X[1], X[3], X[end], x[1]])), \n        [-1.0]\n    ), \n    MOI.Zeros(1)\n)\n\nc2 = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction(\n        MOI.VectorAffineTerm.(1:1, MOI.ScalarAffineTerm.([1., 2, 1, 2, 2, 1, 1, 1], [X; x[2]; x[3]])), \n        [-0.5]\n    ), \n    MOI.Zeros(1)\n)\n\nobjXidx = [1:3; 5:6]\nobjXcoefs = 2*ones(5)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarAffineFunction{Float64}}(),\nMOI.ScalarAffineFunction(MOI.ScalarAffineTerm.([objXcoefs; 1.0], [X[objXidx]; x[1]]), 0.0))\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)\n\nsol = MOI.optimize!(model)\n\n# fetch solution\nx = sol.primal\ns = sol.slack\ny = sol.dual\n\nprintln(\"x -> \", round.(x; digits=3))\nprintln(\"s -> \", round.(s; digits=3))\nprintln(\"y -> \", round.(y; digits=3))\n\n# perturbations in the parameters\ndA = ones(11, 9)\ndb = ones(11)\ndc = ones(9)\n\n# differentiate and get the gradients\ndx, dy, ds = backward_conic!(model, dA, db, dc)\n\nprintln(\"dx -> \", round.(dx; digits=3))\nprintln(\"ds -> \", round.(ds; digits=3))\nprintln(\"dy -> \", round.(dy; digits=3))","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Create a differentiable model from existing optimizers","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    using DiffOpt\n    using SCS\n    \n    model = diff_optimizer(SCS.Optimizer)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Update and solve the model ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    x = MOI.add_variables(model, 2)\n    c = MOI.add_constraint(model, ...)\n    \n    MOI.optimize!(model)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Finally differentiate the model (primal and dual variables specifically) to obtain product of jacobians with respect to problem parameters and a backward pass vector. Currently DiffOpt supports two backends for differentiating a model:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To differentiate Convex Quadratic Program","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"beginalign*\n min_x in mathbbR^n  frac12 x^T Q x + q^T x   \n textst                A x = b        qquad         b in mathbbR^m \n                            G x leq h     qquad         h in mathbbR^p\nendalign*","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"we can use the backward! method","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    grads = backward!(model, [\"Q\", \"q\", \"h\"], [1.0 1.0])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To differentiate convex conic program","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"beginalign*\n min_x in mathbbR^n  c^T x \n textst                A x + s = b  \n                            b in mathbbR^m  \n                            s in mathcalK\nendalign*","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"we can use the backward_conic! method with perturbations in matrices A, b, c","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    grads = backward_conic!(model, dA, db, dc)","category":"page"},{"location":"manual/#Manual","page":"Manual","title":"Manual","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nAs of now, this package only works for optimization models that can be written either in convex conic form or convex quadratic form.","category":"page"},{"location":"manual/#Supported-objectives-and-constraints-scheme-1","page":"Manual","title":"Supported objectives & constraints - scheme 1","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"For QPTH/OPTNET backend (using backward! method), the package supports following Function-in-Set constraints: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function MOI Set\nSingleVariable GreaterThan\nSingleVariable LessThan\nSingleVariable EqualTo\nScalarAffineFunction GreaterThan\nScalarAffineFunction LessThan\nScalarAffineFunction EqualTo","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"and the following objective types: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function\nSingleVariable\nScalarAffineFunction\nScalarQuadraticFunction","category":"page"},{"location":"manual/#Supported-objectives-and-constraints-scheme-2","page":"Manual","title":"Supported objectives & constraints - scheme 2","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"For DiffCP/CVXPY backend (using backward_conic! method), the package supports following Function-in-Set constraints: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function MOI Set\nVectorOfVariables Nonnegatives\nVectorOfVariables Nonpositives\nVectorOfVariables Zeros\nVectorOfVariables SecondOrderCone\nVectorOfVariables PositiveSemidefiniteConeTriangle\nVectorAffineFunction Nonnegatives\nVectorAffineFunction Nonpositives\nVectorAffineFunction Zeros\nVectorAffineFunction SecondOrderCone\nVectorAffineFunction PositiveSemidefiniteConeTriangle","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"and the following objective types: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function\nSingleVariable\nScalarAffineFunction","category":"page"},{"location":"manual/#Creating-a-differentiable-optimizer","page":"Manual","title":"Creating a differentiable optimizer","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can create a differentiable optimizer over an existing MOI solver by using the diff_optimizer utility. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"diff_optimizer","category":"page"},{"location":"manual/#DiffOpt.diff_optimizer","page":"Manual","title":"DiffOpt.diff_optimizer","text":"diff_optimizer(optimizer_constructor)::Optimizer\n\nCreates a DiffOpt.Optimizer, which is an MOI layer with an internal optimizer and other utility methods. Results (primal, dual and slack values) are obtained by querying the internal optimizer instantiated using the  optimizer_constructor. These values are required for find jacobians with respect to problem data.\n\nOne define a differentiable model by using any solver of choice. Example:\n\njulia> using DiffOpt, GLPK\n\njulia> model = diff_optimizer(GLPK.Optimizer)\njulia> model.add_variable(x)\njulia> model.add_constraint(...)\n\njulia> backward!(model)  # for convex quadratic models\n\njulia> backward!(model)  # for convex conic models\n\n\n\n\n\n","category":"function"},{"location":"manual/#Adding-new-sets-and-constraints","page":"Manual","title":"Adding new sets and constraints","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Usage interface DiffOpt models is same as other MOI Optimizers. So the same add_variable, add_constraint utilities can be used.","category":"page"},{"location":"manual/#Projections-on-cone-sets","page":"Manual","title":"Projections on cone sets","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"DiffOpt requires taking projections and finding projection gradients of vectors while computing the jacobians. For this purpose, we use MathOptSetDistances.jl, which is a dedicated package for computing set distances, projections and projection gradients.","category":"page"},{"location":"manual/#Conic-problem-formulation","page":"Manual","title":"Conic problem formulation","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nAs of now, the package is using SCS geometric form for affine expressions in cones.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Consider a convex conic optimization problem in its primal (P) and dual (D) forms:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"beginsplit\nbeginarray llcc\ntextbfPrimal Problem   textbfDual Problem  \nmboxminimize  c^T x  quad quad  mboxminimize  b^T y  \nmboxsubject to  A x + s = b  quad quad  mboxsubject to  A^T y + c = 0 \n s in mathcalK    y in mathcalK^*\nendarray\nendsplit","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"x in R^n is the primal variable, y in R^m is the dual variable, and s in R^m is the primal slack","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"variable","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"mathcalK subseteq R^m is a closed convex cone and mathcalK^* subseteq R^m is the corresponding dual cone","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"variable","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"A in R^m times n, b in R^m, c in R^n are problem data","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"In the light of above, DiffOpt differentiates program variables x, s, y  w.r.t pertubations/sensivities in problem data i.e. dA, db, dc. This is achieved via implicit differentiation and matrix differential calculus.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Note that the primal (P) and dual (D) are self-duals of each other. Similarly for the constraints we support, mathcalK is same in format as mathcalK^*.","category":"page"},{"location":"manual/#Reference-articles","page":"Manual","title":"Reference articles","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Differentiating Through a Cone Program - Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, Walaa M. Moursi, 2019\nA fast and differentiable QP solver for PyTorch. Crafted by Brandon Amos and J. Zico Kolter.\nOptNet: Differentiable Optimization as a Layer in Neural Networks","category":"page"},{"location":"manual/#Backward-Pass-vector","page":"Manual","title":"Backward Pass vector","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"One possible point of confusion in finding jacobians is the role of the backward pass vector - above eqn (7), OptNet: Differentiable Optimization as a Layer in Neural Networks. While differentiating convex programs, it is often the case that we dont't want to find the acutal derivatives, rather we might be interested in  computing the product of jacobians with a backward pass vector, often used in backprop in machine learing/automatic differentiation. This is what happens in scheme 1 of DiffOpt backend.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"But, for the conic system (scheme 2), we provide perturbations in conic data (dA, db, dc) to compute pertubations (dx, dy, dz) in input variables. Unlike the quadratic case, these perturbations are actual derivatives, not the product with a backward pass vector. This is an important distinction between the two schemes of differential optimization.","category":"page"},{"location":"matrix-inversion-manual/#Differentiating-a-QP-wrt-a-single-variable","page":"Differentiating a simple QP by hand","title":"Differentiating a QP wrt a single variable","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Consider the quadratic program","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"beginsplit\nbeginarray ll\nmboxminimize  frac12 x^T Q x + q^T x \nmboxsubject to  G x leq h x in mathcalR^2 h in mathcalR \nendarray\nendsplit","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"where Q, q, G are fixed and h is the single parameter.","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"In this example, we'll try to differentiate the QP wrt h, by finding its jacobian by hand (using Eqn (6) of QPTH article) and compare the results:","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"In python, using CVXPYLayers - https://github.com/cvxgrp/cvxpylayers#tensorflow-2\nIn Julia, using LinearAlgebra, Dualization.jl and MOI","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Assuming ","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Q = [[4, 1], [1, 2]]\nq = [1, 1]\nG = [1, 1]","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"and begining with a starting value of h=-1","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"few values just for reference","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"variable optimal value note\nx* [-0.25; -0.75] Primal optimal\n𝜆∗ -0.75 Dual optimal","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-using-matrix-inversion","page":"Differentiating a simple QP by hand","title":"Finding Jacobian using matrix inversion","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Lets formulate Eqn (6) of QPTH article for our QP. If we assume h as the only parameter and Q,q,G as fixed problem data - also note that our QP doesn't involves Ax=b constraint - then Eqn (6) reduces to ","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"begingather\n beginbmatrix \n     Q  g^T \n     lambda^* g  g z^* - h\n endbmatrix\n beginbmatrix \n     dz \n     d lambda\n endbmatrix\n =\n  beginbmatrix\n   0 \n   lambda^* dh\n   endbmatrix\nendgather","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Now to find the jacobians $ \\frac{\\partial z}{\\partial h}, \\frac{\\partial \\lambda}{\\partial h}$ we substitute dh = I = [1] and plug in values of Q,q,G to get","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"begingather\n beginbmatrix \n     4  1  1 \n     1  2  1 \n     -075  -075  0\n endbmatrix\n beginbmatrix \n     fracpartial z_1partial h \n     fracpartial z_2partial h \n     fracpartial lambdapartial h\n endbmatrix\n =\n  beginbmatrix\n   0 \n   0 \n   -075\n   endbmatrix\nendgather","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Upon solving using matrix inversion, the jacobian is","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"fracpartial z_1partial h = 025 fracpartial z_2partial h = 075 fracpartial lambdapartial h = -175 ","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-in-CVXPYLayers","page":"Differentiating a simple QP by hand","title":"Finding Jacobian in CVXPYLayers","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"import cvxpy as cp\nimport tensorflow as tf\nfrom cvxpylayers.tensorflow import CvxpyLayer\n\nn, m = 2, 1\nx = cp.Variable(n)\nQ = np.array([[4, 1], [1, 2]])\nq = np.array([1, 1])\nG = np.array([1, 1])\nh = cp.Parameter(m)\nconstraints = [G@x <= h]\nobjective = cp.Minimize(0.5*cp.quad_form(x, Q) + q.T @ x)\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dpp()\n\ncvxpylayer = CvxpyLayer(problem, parameters=[h], variables=[x])\nh_tf = tf.Variable([-1.0])  # set a starting value\n\nwith tf.GradientTape() as tape:\n  # solve the problem, setting the values of h to h_tf\n  solution, = cvxpylayer(h_tf)\n\n  summed_solution = tf.math.reduce_sum(solution)\n  \n# note - solution is [-0.25, -0.75]\n#        summed_solution is (-0.25) + (-0.75)\n\n# cvxpylayers allows gradient of the summed solution only, with respect to h\ngradh = tape.gradient(summed_solution, [h_tf])","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-using-MOI,-Dualization.jl,-LinearAlgebra.jl","page":"Differentiating a simple QP by hand","title":"Finding Jacobian using MOI, Dualization.jl, LinearAlgebra.jl","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"using Random\nusing MathOptInterface\nusing Dualization\nusing OSQP\nusing LinearAlgebra\n\nconst MOI = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;\n\nn = 2 # variable dimension\nm = 1; # no of inequality constraints\n\nQ = [4. 1.;1. 2.]\nq = [1.; 1.]\nG = [1. 1.;]\nh = [-1.;]   # initial values set\n\n\n# create the optimizer\nmodel = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nx = MOI.add_variables(model, n);\n\n# define objective\nquad_terms = MOI.ScalarQuadraticTerm{Float64}[]\nfor i in 1:n\n    for j in i:n # indexes (i,j), (j,i) will be mirrored. specify only one kind\n        push!(quad_terms, MOI.ScalarQuadraticTerm(Q[i,j],x[i],x[j]))\n    end\nend\n\nobjective_function = MOI.ScalarQuadraticFunction(MOI.ScalarAffineTerm.(q, x),quad_terms,0.)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarQuadraticFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)\n\n# add constraint\nMOI.add_constraint(\n    model,\n    MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(G[1,:], x), 0.),\n    MOI.LessThan(h[1])\n)\n\n# solve\nMOI.optimize!(model)\n\n# sanity-check\n@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]\n\nx̄ = MOI.get(model, MOI.VariablePrimal(), x)\n\n# obtaining λ*\n\njoint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;\n\n# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);\n\nmap = primal_dual_map.primal_con_dual_var\n\nfor con_index in keys(map)\n    λ = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    println(λ)\nend\n\nLHS = [4 1 1; 1 2 1; 1 1 0]  # of Eqn (6)\nRHS = [0; 0; 1]  # of Eqn (6)\n\npp \\ qq  # the jacobian","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"3-element Array{Float64,1}:\n  0.25\n  0.75\n -1.75","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [DiffOpt]","category":"page"},{"location":"reference/#DiffOpt.Dπ-Tuple{Any,Any}","page":"Reference","title":"DiffOpt.Dπ","text":"Dπ(cones::Array{<: MOI.AbstractVectorSet}, v::Vector{Vector{Float64}})\n\nFind gradient of projection of vectors in v on product of cones. For more info, refer https://github.com/matbesancon/MathOptSetDistances.jl\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.backward!-Tuple{Optimizer,Array{String,N} where N,Array{Float64,N} where N}","page":"Reference","title":"DiffOpt.backward!","text":"backward!(model::Optimizer, params::Array{String}, dl_dz::Array{Float64})\n\nMethod to differentiate optimal solution z and return product of jacobian matrices (dz / dQ, dz / dq, etc) with  the backward pass vector dl / dz\n\nThe method computes the product of \n\njacobian of problem solution z* with respect to   problem parameters params recieved as method arguments\na backward pass vector dl / dz, where l can be a loss function\n\nNote that this method does not returns the actual jacobians.\n\nFor more info refer eqn(7) and eqn(8) of https://arxiv.org/pdf/1703.00443.pdf\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.backward_conic!-Tuple{Optimizer,Array{Float64,2},Array{Float64,N} where N,Array{Float64,N} where N}","page":"Reference","title":"DiffOpt.backward_conic!","text":"backward_conic!(model::Optimizer, dA::Array{Float64,2}, db::Array{Float64}, dc::Array{Float64})\n\nMethod to differentiate optimal solution x, y, s given perturbations related to  conic program parameters A, b, c. This is similar to backward! method but it this does returns the actual jacobians.\n\nFor theoretical background, refer Section 3 of Differentiating Through a Cone Program, https://arxiv.org/abs/1904.09043\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.create_LHS_matrix","page":"Reference","title":"DiffOpt.create_LHS_matrix","text":"create_LHS_matrix(z, λ, Q, G, h, A=nothing)\n\nInverse matrix specified on RHS of eqn(7) in https://arxiv.org/pdf/1703.00443.pdf\n\nHelper method while calling backward!\n\n\n\n\n\n","category":"function"},{"location":"reference/#DiffOpt.diff_optimizer-Tuple{Any}","page":"Reference","title":"DiffOpt.diff_optimizer","text":"diff_optimizer(optimizer_constructor)::Optimizer\n\nCreates a DiffOpt.Optimizer, which is an MOI layer with an internal optimizer and other utility methods. Results (primal, dual and slack values) are obtained by querying the internal optimizer instantiated using the  optimizer_constructor. These values are required for find jacobians with respect to problem data.\n\nOne define a differentiable model by using any solver of choice. Example:\n\njulia> using DiffOpt, GLPK\n\njulia> model = diff_optimizer(GLPK.Optimizer)\njulia> model.add_variable(x)\njulia> model.add_constraint(...)\n\njulia> backward!(model)  # for convex quadratic models\n\njulia> backward!(model)  # for convex conic models\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.generate_lp-Tuple{Any,Any,Any}","page":"Reference","title":"DiffOpt.generate_lp","text":"Generates a non-trivial random MOI linear program by adding variables and constraints to MOI compatible Optimizer optimizer\n\nminimize c' * x subject to Ax <= b, x >= 0 where x in R^{n}, A in R^{m*n}, b in R^{m}, c in R^{n}\n\nNote: Mutates the optimizer object\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.generate_qp-NTuple{4,Any}","page":"Reference","title":"DiffOpt.generate_qp","text":"Generates a non-trivial random MOI convex quadratic program  by adding variables and constraints to MOI compatible Optimizer optimizer\n\nminimize 0.5 * x' * Q * x  + q' * x subject to Gx <= h, Ax == b where x in R^{n}, Q in R^{n*n}, q in R^{n}, G in R^{m,n}, h in R^{m}, A in R^{p*n}, b in R^{p}\n\nNote: (1) Mutates the optimizer object       (2) Matrix Q is Positive Semidefinite\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.get_problem_data-Tuple{MathOptInterface.AbstractOptimizer}","page":"Reference","title":"DiffOpt.get_problem_data","text":"get_problem_data(model::MOI.AbstractOptimizer)\n\nReturn problem parameters as matrices along with other program info such as number of constraints, variables, etc\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.π-Tuple{Any,Any}","page":"Reference","title":"DiffOpt.π","text":"π(cones::Array{<: MOI.AbstractVectorSet}, v::Vector{Vector{Float64}})\n\nFind projection of vectors in v on product of cones. For more info, refer https://github.com/matbesancon/MathOptSetDistances.jl\n\n\n\n\n\n","category":"method"},{"location":"#DiffOpt.jl","page":"Home","title":"DiffOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status)  (Image: Coverage Status) (Image: AppVeyor Status) (Image: Docs status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"DiffOpt is a package for differentiating convex optimization program (JuMP.jl or MathOptInterface.jl models) with respect to program parameters. Note that this package does not contains any solver. This package has two major backends, available via backward! and backward_conic! methods, to differentiate models with optimal solutions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nCurrently supports linear programs, convex quadratic programs and convex conic programs (SDP, SOCP constraints only). ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiffOpt can be installed through the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(v1.3) pkg> add https://github.com/AKS1996/DiffOpt.jl","category":"page"},{"location":"#Why-are-Differentiable-optimization-problems-important?","page":"Home","title":"Why are Differentiable optimization problems important?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Differentiable optimization is a promising field of convex optimization and has many potential applications in game theory, control theory and machine learning (specifically deep learning - refer this video for more). Recent work has shown how to differentiate specific subclasses of convex optimization problems. But several applications remain unexplored (refer section 8 of this really good thesis). With the help of automatic differentiation, differentiable optimization can a significant impact on creating end-to-end systems for modelling a neural network, stochastic process, or a game.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions to this package are more than welcome, if you find a bug or have any suggestions for the documentation please post it on the github issue tracker.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When contributing please note that the package follows the JuMP style guide","category":"page"}]
}
